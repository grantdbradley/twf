
% </A>
% </A>
% </A>
\week{December 25, 2006 }

Today I'd like to talk a bit about the first stars in the Universe,
and some hotly contested possible observations of these stars.  Then I
want to describe a new paper by my student Derek Wise.  But first - if
anyone gave you a gift certificate for a bookstore this holiday
season, here are two suggestions.

The first one is really easy and fun:

1) William Poundstone, Fortune's Formula: The Untold Story of the 
Scientific Betting System that Beat the Casinos and Wall Street, 
Farrar, Strauss and Giroux, New York, 2005.

Packed with rollicking tales of gangsters, horse-racing, blackjack, 
and insider 
trading, this is secretly the story of how Claude Shannon developed
information theory - and how he and his sidekick John Kelly Jr. used 
it to make money in casinos and Wall Street.  I'd known about Shannon's 
work on information... but not that he beat 99.9% of mutual fund 
managers, making an average compound return of 28% for many years -
as compared to 27% for Warren Buffett!

This book has just a few equations in it.  I was delighted by one 
discovered by Kelly, which I'd never seen before.  Translating into my
own favorite notation, it goes like this:

S = log M

It's the fundamental equation relating gambling to information!
Let me explain it - in language far more complicated than you'll 
see in Poundstone's book.

What's M?  It's the best possible average growth of a gambler's money.  
For example, if his best possible strategy lets him triple his money 
on average, then M = 3.  

What's S?  This is the amount of "inside information" the
gambler has: information he has, that the people he's betting against
don't.

Some technical stuff: First, the above "average" is a
geometric mean, not an arithmetic mean.  Second, if we measure
information in bits, we need to use base 2 in the logarithm.
Physicists would probably prefer to use base e, which means measuring
information in "nits".  It doesn't really matter, but let's
use base 2 for now.

To get a feeling for why Kelly's theorem is true, it's best to start 
with the simplest example.  If S = 1, then M = 2. So, if a gambler 
receives one bit of inside information, he can double his money!

This sounds amazing, but it's also obvious.  

Suppose you have one bit of inside information: for example, whether a
flipped coin will land heads up or tails up.  Then you can make a bet
with somebody where they give you $1,000,000 if you guess the coin
flip correctly, and you give them $1,000,000 if you guess wrong.  This
is a fair bet, so they will accept.  That is, they'll \emph{think}
it's fair if they don't suspect you have inside information!  But
since you do have this information, you'll win the bet, and double
your money on this coin flip.

Kelly's equation is usually phrased in terms of the \emph{rate} at
which the gambler gets inside information, and the \emph{rate} at
which his money grows.  So, for example, to earn 12% interest
annually, you only need to receive

log(1.12) = 0.163

bits of inside information - and find some dupe willing to make bets
with you about this.  

The last part is the hard part: the "inside information"
really needs to be information people don't believe you have.  I must
learn hundreds of bits of information about math each year - stuff
only I know - but I haven't found anyone simultaneously smart enough
to understand it and dumb enough to make bets with me about it!

Still, I like this relation between information theory and gambling,
because one stream of Bayesian probability theory says probabilities 
are subjectively defined in terms of the bets you would accept.  

The argument for this is called the "Dutch book argument".
It basically shows how you can make money off someone who makes bets
in ways that correspond to stupid probabilities that don't add to 1,
or fail to be coherent in other ways:

2) Carlton M. Caves, Probabilities as betting odds and the Dutch book,
available at <a href = "http://info.phys.unm.edu/~caves/reports/dutchbook.pdf">http://info.phys.unm.edu/~caves/reports/dutchbook.pdf</a>

So, there's a deep relation between gambling and probability - no news 
here, really.

But, there's also a deep relation between probability and information 
theory, discovered by Shannon.  Briefly, it goes like this: the 
information you obtain by learning the value of a random variable is



% parser failed at source line 146
