
% </A>
% </A>
% </A>
\week{January 1, 2010 }


Happy New Decade!  I hope you're doing well.  This week I'll say more
about rational homotopy theory, and why the difference between
equality and isomorphism is important for understanding the weather 
in space.  But first: electrical circuits!

But even before that... guess what this is a picture of:

<div align = "center">
<img border = "2" src = "http://math.ucr.edu/home/baez/linear_dunes.jpg">
</div>

Now, what about electrical circuits?

I've been thinking This Week's Finds has become a bit too far removed
from its roots in physics.  This problem started when I quit working
on quantum gravity and started focusing on n-categories.  Overall it's
been a big boost to my sanity.  But I don't want This Week's Finds to
be comprehensible only to an elite coterie of effete mathematicians -
the sort who eat simplicial presheaves for breakfast and burp up
monoidal bicategories.

So, in an effort to prevent This Week's Finds from drifting off into
the stratosphere of abstraction, I've decided to talk a bit about
electrical circuits.  Admittedly, these are less glamorous than
theories of quantum gravity.  But: they actually work!  And there is 
a lot of nice math involved.

I rarely dare predict what \emph{future} Week's Finds will discuss, because
I know from bitter experience that I change my mind.  But lately I've
started writing a new way: long stories with lots of episodes, which I
can dole out a bit at a time.  So I know that for at least a few Weeks
I'll talk about electrical circuits - and various related things.

<div align = "center">
<a href = "http://www.free-circuits.com/circuits/audio/135/10w-audio-amplifier-with-bass-boost">
<img border = "none" src = "electronics_circuit_diagram_10W_amplifier_with_bass_boost.gif">
% </a>
<font size = "-1">
<br/> 10 watt amplifier with bass boost </font>
</div>

I've been trying to understand electrical circuits using category
theory for a long time.  Indeed, Peter Selinger and I are very slowly
writing a paper on this subject.  The basic inspiration is that
electrical circuit diagrams look sort of like Feynman diagrams, flow
charts, and various other diagrams that have "inputs" and
"outputs".  I love diagrams like this!  All the kinds I've
met so far can be nicely formalized using category theory.  For an
explanation, try this:

1) John Baez and Mike Stay, Physics, topology, logic and computation:
a Rosetta Stone, to appear in New Structures in Physics, ed. Bob
Coecke.  Available at <a href = "http://arxiv.org/abs/arXiv:0903.0340">arXiv:0903.0340</a>.

And after I spent a while thinking about electrical circuits using
category theory, I realized that this perspective might shed light on
analogies between circuits and other systems.

For example: mechanical systems made from masses and springs!  

Indeed, whenever I teach linear differential equations, I like to
explain the basic equation describing a "damped harmonic
oscillator": for example, a rock hanging on a spring.

<div align = "center">
<img src = "damped_spring.gif">
</div>

Then I explain how the same equation describes the simplest circuit
made of a resistor, an inductor, and a capacitor - the so-called
"RLC circuit".  It's a nice easy example of how the same
math applies to superficially different but secretly isomorphic
problems!

Let me explain.  I hope this is a chance to help mathematicians review
their physics and ask questions about it over on the <i>n</i>-Category 
Caf&eacute;.

Let the height of a rock hanging on a spring be q(t) at time t, where
q(t) is negative when the end of the spring is down below its
equilibrium position.  Then making all sort of simplifying assumptions
and approximations, we have:

m q"(t) = - c q'(t) - k q(t) + F(t)

where:

<ul>
<li>
   m is the <a href = "http://en.wikipedia.org/wiki/Mass">mass</a> of the rock.
</li>
<li>
   c is the <a href = "http://en.wikipedia.org/wiki/Damping">damping 
   coefficient</a>, which describes the force due to
   friction: we're assuming this force is proportional to the rock's
   velocity, but points the other way.
</li>
<li>
   k is the <a href = "http://en.wikipedia.org/wiki/Hooke%27s_law">spring 
   constant</a>, which describes the force due to the
   spring: we're assuming this force is proportional to how much the
   spring is stretched from its equilibrium position, but points the
   other way.
</li>
<li>
   F(t) is the externally applied <a href =
   "http://en.wikipedia.org/wiki/Force">force</a>, e.g. if you push on
   the rock.  
</li> 
</ul>

This equation is just <a href =
"http://en.wikipedia.org/wiki/Newton%27s_laws_of_motion#Newton.27s_second_law">Newton's
second law</a>, force equals mass times acceleration.  The left side
of the equation is mass times acceleration; the right side is the
total force.

Now for the analogy.  Everything here is analogous to something in an
RLC circuit!  An <a href =
"http://en.wikipedia.org/wiki/RLC_circuit#Series_RLC_with_Th.C3.A9venin_power_source">RLC
circuit</a> has current flowing around a loop of wire with 4 gizmos on
it: a resistor, an inductor, a capacitor, and a voltage source - for
example, a battery.

<div align = "center">
<img src = "RLC_series_circuit.png">
</div>



I won't say much about these gizmos.  I just want to outline the
analogy.  The amount of current is analogous to the velocity of the
rock, so let's call it q'(t).  The resistor acts to slow the current
down, just as friction acts to slow down the rock.  The inductor is
analogous to the mass of the rock.  The capacitor is analogous to the
spring - but according to the usual conventions, a capacitor with a
big "capacitance" acts like a weak spring.  Finally, the
voltage source is analogous to the external force.

So, here's the equation that governs the RLC circuit:

L q"(t) = - R q'(t) - (1/C) q(t) + V(t)
where

<ul>
<li>
L is the <a href =
"http://en.wikipedia.org/wiki/Inductance">inductance</a> of the
<a href = "http://en.wikipedia.org/wiki/Inductor">inductor</a>.  

<div align = "center">
<img src = "electronics_inductor_symbol.png">
<br/>
<font size = "-1">
inductor
</font>
</div>

</li>
<li>
R is the <a href =
"http://en.wikipedia.org/wiki/Electrical_resistance">resistance</a> of the
<a href = "http://en.wikipedia.org/wiki/Resistor">resistor</a>.
<div align = "center">
<img src = "electronics_resistor_symbol.png">
<br/>
<font size = "-1">
resistor
</font>
</div>

</li>
<li>
C is the <a href =
"http://en.wikipedia.org/wiki/Capacitance">capacitance</a> of the
<a href = "http://en.wikipedia.org/wiki/Capacitor">capacitor</a>.
<div align = "center">
<img src = "electronics_capacitor_symbol.png">
<br/>
<font size = "-1">
capacitor
</font>
</div>

</li>
<li>
V is the <a href =
"http://en.wikipedia.org/wiki/Voltage">voltage</a> of the <a href = "http://en.wikipedia.org/wiki/Voltage_source">voltage source</a>.
<div align = "center">
<img src = "electronics_voltage_source_symbol.jpg">
<br/>
<font size = "-1">
voltage source
</font>
</div>
</li> </ul>

As you can see, the equation governing the RLC circuit is the same as
the one that governs a rock on a spring!  True, 1/C plays the role of
k, since a capacitor with a big capacitance acts like a spring with a
small spring constant.  But this is just a difference in conventions.
The systems are isomorphic!

We could have fun solving the above equation and pondering what the 
solutions mean, but that would be the class I teach.  Instead,
I want to explain how this famous analogy between mechanics and 
electronics is just one of many analogies.

When I started thinking seriously about electrical circuits and
category theory, I mentioned them my student Mike Stay, and he
reminded me of the "hydraulic analogy" where you think of an
electrical current flowing like water through pipes.  There's a 
decent introduction to this here:

2) Wikipedia, Hydraulic analogy, 
<a href = "http://en.wikipedia.org/wiki/Hydraulic_analogy">http://en.wikipedia.org/wiki/Hydraulic_analogy</a>
 
Apparently this analogy goes back to the early days when people were
struggling to understand electricity, before electrons had been
observed.  The famous electrical engineer Oliver Heaviside pooh-poohed
this analogy, calling it the "drain-pipe theory".  I think
he was making fun of William Henry Preece.  Preece was another
electrical engineer, who liked the hydraulic analogy and disliked
Heaviside's fancy math.  In his inaugural speech as president of the
Institution of Electrical Engineers in 1893, Preece proclaimed:

\begin{quote}
  True theory does not require the abstruse language of mathematics to 
  make it clear and to render it acceptable.   All that is solid and 
  substantial in science and usefully applied in practice, have been 
  made clear by relegating mathematic symbols to their proper store 
  place - the study.
\end{quote}
    

According to the judgement of history, Heaviside made more progress in
understanding electromagnetism than Preece.  But there's still a nice
analogy between electronics and hydraulics.

In this analogy, a pipe is like a wire.  Water is like electrical
charge.  The flow of water plays the role of "current".
Water pressure plays the role of "voltage".

A resistor is like a narrowed pipe:

<div align = "center">
<img src = "electronics_analogy_reduced_pipe_resistor.png">
</div>

An inductor is like a heavy turbine placed inside a pipe: this makes
the water tend to keep flowing at the same rate it's already flowing!
In other words, it provides a kind of "inertia", analogous
to the mass of our rock.  Finally, a capacitor is like a tank with
pipes coming in from both ends, and a rubber sheet dividing it in two
lengthwise:

<div align = "center">
<img src = "electronics_analogy_flexible_tank_capacitor.png">
</div>

When studying electrical circuits as a kid, I was shocked when I first
learned that capacitors <i>don't let the electrons through</i>.
Similarly, this gizmo doesn't let the water through.

Okay... by now you're probably wanting to have the analogies laid out
more precisely.  So that's what I'll do.  But I'll throw in one more!
I've been talking about the mechanics of a rock on a spring, where the
motion of the rock up and down is called \emph{translation}.  But we can
also study \emph{rotation} in mechanics.  And then we get these analogies:

<PRE><b>
                displacement    flow          momentum      effort
                     q           q'              p            p'

Mechanics       position       velocity       momentum      force
(translation)

Mechanics       angle          angular        angular       torque
(rotation)                     velocity       momentum

Electronics     charge         current        flux          voltage
                                              linkage

Hydraulics      volume         flow           pressure      pressure
                                              momentum
</b></PRE>

The top row lists 4 concepts from the theory of general systems, and
my favorite symbols for them, where the prime stands for time
derivative - if I could, I'd use a dot.  The other rows list what
these concepts are called in the subjects listed.  So,
"displacement" is the general concept which people call
"position" in the mechanics of translation.  Similarly,
"flow" and "effort" correspond to
"velocity" and "force", while "momentum"
is just "momentum".

I found this chart here:

3) Dean C. Karnopp, Donald L. Margolis and Ronald C. Rosenberg,
System Dynamics: a Unified Approach, Wiley, New York, 1990.

I discovered this wonderful book after an intensive search for stuff
that makes the analogies between mechanics, electronics and hydraulics
explicit.  It turns out there's a whole theory devoted to precisely
this!  It's sometimes called "systems theory" or
"network theory".  Engineers use this theory to design and
analyze systems made out of mechanical, electronic, and/or hydraulic
components: springs, gears, levers, pulleys, pumps, pipes, motors,
resistors, capacitors, inductors, transformers, amplifiers, and more!

Engineers often describe such systems using a notation called
"bond graphs".  Bond graphs are reminscent of Feynman
diagrams... so they're simply \emph{begging} to be understood as a
branch of applied category theory.  In fact, there's an interesting
blend of category theory, symplectic geometry and complex analysis at
work here.  So in the Weeks to come, I'd like to tell you more about
bond graphs and analogies between different kinds of systems.

(I'll warn you right now that Karnopp, like most experts on systems
theory, use the symbols "f" and "e" for flow and effort, instead of q'
and p'.  It's more or less impossible to find a unified notation for
general systems that doesn't conflict with some existing notation
used in the study of some \emph{particular} kind of system.  But since I
want to get into symplectic geometry, I want to use some notation that
reminds me of that - and for physicists, symplectic geometry is the
study of "conjugate variables" like position q and momentum p.)

Okay... enough of this for now.  

Last week I introduced the differential graded commutative algebra
approach to rational homotopy theory.  Next week I'll get into the
differential graded Lie algebra approach, filling in another corner 
of the triangle here:

\begin{verbatim}
                      RATIONAL SPACES
                         /      \  
                        /        \  
                       /          \  
                      /            \
                     /              \
      DIFFERENTIAL GRADED ------- DIFFERENTIAL GRADED
      COMMUTATIVE ALGEBRAS           LIE ALGEBRAS
\end{verbatim}
    
But I realized there's some important stuff I can tell you 
before we get to that!  

Last time I told you how Sullivan defined "rational differential
forms" for any topological space X:

  First he converted this space into a simplicial set Sing(X).  

  Then he defined an algebra A of functions that are polynomials 
  with rational coefficients on each simplex of Sing(X).  

  Then he defined his algebra \Omega (A) of rational differential forms,
  using a general recipe that takes a commutative algebra A and spits
  out a differential graded commutative algebra.

But towards the end, I admitted that homotopy theorists feel perfectly
fine about studying simplicial sets rather than topological spaces.
The reason is that both the category of simplicial sets
and the category of topological spaces are "<a href = 
"http://ncatlab.org/nlab/show/model+category">model categories</a>" - 
contexts where you can do homotopy theory.  
Moreover, these model categories are "<a href = 
"http://ncatlab.org/nlab/show/Quillen+equivalence">Quillen 
equivalent</a>" - the same in every way that
matters for homotopy theory!  Don't worry too much if you don't know
about model categories and Quillen equivalence.  The point is that we
have a functor that converts spaces into simplicial sets:

Sing: [topological spaces] \to  [simplicial sets]

and its right adjoint going back the other way, called "geometric
realization":

| &nbsp; | : [simplicial sets] \to  [topological spaces]


which we also saw last time.  And, these let us freely switch
viewpoints between topological spaces and simplicial sets.

So, while in "<a href = "week286.html">week286</A>" I
defined rational spaces to be specially nice <em>topological
spaces</em>, I could equally well have defined them to be specially
nice \emph{simplicial sets}.  Taking this viewpoint, we can forget
about topological spaces, and think of Sullivan's innovation as a
recipe for defining rational differential forms on a <em>simplicial
set</em>.

This is a good idea.  Among other things, it helps us see more simply
what was so new about rational differential forms when Sullivan first
discovered them.

What's new is that they give a functor that takes any simplicial set S
and gives a differential graded algebra that's \emph{commutative} and whose
cohomology is the rational cohomology of S.  

(By which I mean: the rational cohomology of the space |S|.)

You see, it's not so hard to achieve this if we drop our insistence
that our differential graded algebra be \emph{commutative}.  This has been
known for a long time.  You start with your simplicial set S and
define a "rational n-cochain" on it to be a function that eats
n-simplices and spits out rational numbers.  This gives a cochain
complex
         
$$
       d          d          d
C^{0}(S) ---> C^{1}(S) ---> C^{2}(S) ---> ...
$$
    

where C^{n}(S) is the vector space of rational n-cochains.
This cochain complex is usually called C*(S), where the star stands
for the variable n.  And there's a standard way to make C*(S) into a
differential graded algebra, using a product called the "cup
product".  But, it's not a differential graded
\emph{commutative} algebra.

Instead, it's only graded commutative "up to chain
homotopy".  So, we don't have

v &cup; w = (-1)^{pq} w &cup; v

when v is in C^{p}(S) and w is in C^{q}(S).  But, we do have 

v &cup; w = (-1)^{pq} w &cup; v + da(v,w)

where a(v,w) is something that depends on v and w.  This is good
enough to imply that when we take the cohomology of our cochain
complex, we get a graded commutative algebra.  This algebra is called
H*(S), and the product in here is also called the cup product.  You
can read a lot about it in basic books on algebraic topology.  Here's
one that's free online:

4) Allen Hatcher, Algebraic Topology, Section 3.2: Cup Product, 
available at <a href = "http://www.math.cornell.edu/~hatcher/AT/ATch3.pdf">http://www.math.cornell.edu/~hatcher/AT/ATch3.pdf</a>

The memorably numbered Theorem 3.14 says the cup product is graded
commutative.

So you might say: "So, who cares if the cup product of cochains is
graded commutative merely up to chain homotopy?  When we go to
cohomology, that distinction washes away!"

Well, it turns out there can be lots interesting information in this
chain homotopy a(v,w).  At least, this is true when we do cohomology
using the integers or the integers modulo some prime as our
coefficients - instead of rational numbers, as we've been doing.

In fact this chain homotopy is the tip of an enormous iceberg!  For
starters, it satisfies an interesting equation, but only up chain
homotopy...  and that chain homotopy satisfies an equation of its own,
but only up to homotopy, and so on.  So, we get a differential graded
algebra that's graded commutative up to an infinite series of chain
homotopies.  Folks call this sort of gadget an
"E_{\infty }-algebra".

And when we work over the integers mod some prime, we can squeeze
interesting information out of all these chain homotopies.  They're
called "Steenrod operations".  You can use them to distinguish spaces
that would be indistinguishable if you merely used their cohomology as
a graded commutative algebra!

At least, that's what they say.  I don't \emph{personally} run
around using Steenrod operations to distinguish weird spaces 
that shady characters pull out of their coat pockets on dark
streetcorners.  Some topologists actually do.  But what fascinates me
is the subtle distinction between equations that hold "on the
nose" and equations that hold only up to homotopy, or up to
isomorphism.  Sometimes you can "strictify" a gadget where
the equations hold only up to homotopy, and get them to hold on the
nose.  But sometimes you can't.

Once I was giving a talk about n-categories and Roger Penrose was
in the audience.  I said the most basic fact about n-categories was:

<div align = "center">
          &cong; &ne; =
</div>

He raised his hand and asked:

<div align = "center">
          &cong; &cong; = ?
</div>

Very good question!  And the answer is: sometimes yes, sometimes no.
This is where things get interesting!

So, it's a famous puzzle whether you can find some functorial way to 
turn a simplicial set S into a differential graded commutative algebra
A*(S) whose cohomology is the usual cohomology H*(S).  This is called the 
"commutative cochain problem".

I haven't said it precisely enough yet, since there's a cheap and easy
way to solve the version I just stated: just A*(S) to be the cohomology
H*(S) itself, with d = 0.  What a dirty trick!

To rule out such tricks people demand various extra good properties.  
For example, the usual cochains C*(S) are "extendible": any cochain
on a little simplicial set extends to a cochain on a bigger one.  
In other words, if 

S \to  T

is an inclusion of simplicial sets, then the corresponding map 

C*(T) \to  C*(S)

is onto.  This is definitely not true if we replace C* by H*.  

This paper gives a bit of history of the commutative cochains problem:

5) Bohumil Cenkl, Cohomology operations from higher products in the de
Rham complex, Pacific J. Math. 140 (1989), 21-33.  Available at
<a href = "http://projecteuclid.org/euclid.pjm/1102647247">http://projecteuclid.org/euclid.pjm/1102647247</a>

It gives a somewhat different statement of the problem, which alas I
don't understand, and it proves that this version has no solution if
we work over the integers.  But over the rationals it \emph{does}, if we
take A*(S) to be the rational differential forms on our simplicial set
S.

Just so you don't think this is pie-in-the-sky stuff, I should
emphasize that this problem actually matters in electrical
engineering, where we might triangulate spacetime and study discrete
analogues of famous differential equations on the resulting simplicial
complex!  My friends Robert Kotiuga and Eric Forgy have thought about
this a lot.  

Here's a nice excerpt from the website of a
conference at Boston University.  I bet Robert Kotiuga wrote this.  It
mentions "Whitney forms", which are simplex-wise linear
differential forms on a simplicial set.  These are closely related to
Sullivan's simplex-wise <i>polynomial</i> differential forms.

\begin{quote}
    The analysis of electric circuits, using <a href = "http://en.wikipedia.org/wiki/Kirchhoff%27s_circuit_laws">Kirchhoff's Laws</a>, brought
   topology into electrical engineering over 150 years ago.  Hermann
   Weyl's reformulation of Kirchhoff's laws in terms of homology over
   80 years ago is an abstraction which is proving to be essential in
   the finite element analysis of three-dimensional electromagnetic
   fields.  It enables computers to be programmed to identify an
   electrical circuit in an electromagnetic field problem - a task once
   considered the domain of the engineer's intuition. In &quot;control
   theory&quot; parlance, circuit theory equations are low frequency model
   reductions of distributed parameter electromagnetic systems, and
   homology theory yields the key mathematical tools for obtaining robust
   numerical algorithms.  One aspect of the workshop will deal with
   large scale homology calculations and the realization of cycles
   representing generators of integral homology groups as embedded
   manifolds.  The underlying homology calculations involve large sparse
   integer matrices with remarkable structure even when the underlying
   finite element meshes are &quot;unstructured&quot;.  One aim of the workshop
   is to bring together those performing large scale homology
   calculations in the context of dynamical systems and point cloud
   data analysis, with those requiring more geometrical applications
   of homology groups in electromagnetics.

   Over two decades ago, boundary value problems arising in the
   analysis of quasistatic electromagnetic fields were reinterpreted
   in terms of Hodge theory on manifolds with boundary.  This observation
   is quite natural when Maxwell's equations are viewed in the context of
   differential forms and the problem of defining potentials is
   phrased in terms of de Rham cohomology.  This observation, along
   with the variational formulation of Hodge theory on manifolds with
   boundary, created a revolution in the finite element analysis of
   electromagnetic fields.  When phrased this way, the most difficult
   theoretical problems were actually solved in the 1950's by Andre Weil
   and Hassler Whitney who were concerned with problems in algebraic
   topology. They had an explicit interpolation formula for turning
   simplicial cochains into piecewise linear differential forms.  This
   formula gives a chain homotopy between the algebraic complexes
   involved, and an isomorphism of cohomology rings.  Although it took
   30 years for Whitney forms to impact engineering practice, once the
   genie was out of the bottle, there was no way to put it back in.
   In the early 1990s, Whitney form techniques solved the problem of
   &quot;spurious modes&quot; appearing in electromagnetic cavity resonator
   calculations and soon after became widely accepted as an essential
   tool which is only recently being appreciated in the context of
   nanophotonics.

   It is important to re-examine this Whitney form revolution in the
   context of recent attempts to develop &quot;discrete exterior
   calculus,&quot; &quot;mimetic discretizations,&quot;
   &quot;compatible discretizations&quot; etc. For example, in
   algebraic topology it is well known that simplicial cochains do not
   admit a graded-commutative, associative product analogous to the
   wedge product on differential forms. This classical result, known
   as &quot;the commutative cochain problem,&quot; is surprising and
   unintuitive in light of the fact that simplicial cochains admit a
   graded-commutative, associative product on the level of cohomology,
   analogous to the one induced by the wedge product in the de Rham
   complex. The bottom line is that these types of classical results
   are often ignored by newcomers trying to develop a discrete
   approach to calculus.  Obviously, there is still some important
   technology transfer to be performed between algebraic topology and
   numerical analysis!  Much of the mathematical work was done by
   Patodi, Dodziuk and Muller in the 1970's, has been exploited by
   electrical engineers, but has been largely ignored by applied
   mathematicians.  Although the multiplicative structure on
   differential forms does not seem to be very important in the
   context of linear boundary value problems, it seems to play an
   important role in magnetohydrodynamics.  Magnetohydrodynamics, in
   turn is an essential tool in space physics, in particular, in the
   growing field of space weather.
\end{quote}
    

So you see, everything is related.  The difference between equality
and isomorphism matters when you're trying to simulate the weather in
space!  That's the kind of thing that makes math so fun.  Here's the
conference webpage:

5) Advanced Computational Electromagnetics 2006 (ACE 'O6), Boston
University, <a href = "http://www.bu.edu/eng/ace2006/">http://www.bu.edu/eng/ace2006/</a>

You can learn more here:

6) P. W. Gross and P. Robert Kotiuga, Electromagnetic Theory and
Computation: A Topological Approach, Cambridge University Press,
2004. 

Someday my discussion of electrical circuits may expand to include
some algebraic topology.  But all I want to explain now is the usual
cup product on the cochains C*(S) for a simplicial set S.  We'll 
need this in the Weeks to come!

Actually, it'll be easier if I work with chains instead of cochains.
For us a chain complex will be a list of vector spaces and linear maps

$$
     d       d      d
C_{0} <--- C_{1} <--- C_{2} <--- ...
$$
    

with d^{2} = 0.  We call the whole thing
C_{<sub>*}</sub>, where now the star is a subscript.  I'll
show you the usual way to get a chain complex
C_{<sub>*}</sub>(S) from a simplicial set, and then show you a
way to \emph{comultiply} chains.  Then you can get the cochains by
taking duals:

C^{n}(S) = C_{n}(S)*

This will give a way to multiply chains.

Here's how it goes.  The idea is that we define the comultiplication
directly at the level of simplicial sets and then feed it through a
couple of functors.  There's a functor 

F: [simplicial sets] \to  [simplicial vector spaces]

and a functor

N: [simplicial vector spaces] \to  [chain complexes]

Composing these will give the chains C_{<sub>*}</sub>(S) for a simplicial set S.

The first functor 

F: [simplicial sets] \to  [simplicial vector spaces]

creates the free simplicial vector space on a simplicial set.  
This functor is symmetric monoidal: it carries products of simplicial
spaces to tensor products of simplicial vector spaces.  The second
functor

N: [simplicial vector spaces] \to  [chain complexes]

is called the "normalized chain complex" or "normalized Moore
complex" functor.  This functor is an equivalence of categories! 
It's \emph{almost} symmetric monoidal, but not quite, and this is 
where all the subtlety lies.

The category of simplicial sets has finite products.  So, every
simplicial set has a diagonal map:

\Delta : S \to  S \times  S

It also a unique map to the simplicial set called 1, which 
consists of single 0-simplex:

\epsilon : S \to  1

These two maps satisfy the usual axioms of a commutative monoid,
written out as commutative diagrams, except with the arrows pointing
backwards.  So, S is a "cocommutative comonoid" in the category of
simplicial sets.  

Indeed, whenever you have any category with finite products, every
object in it becomes a cocommutative comonoid - and in a unique way!

So far, this is a completely bland fact of life.
If we feed our simplicial set S through the functor

F: [simplicial sets] \to  [simplicial vector spaces]

what happens?  Well, because this functor is monoidal it sends
comonoids to comonoids.  And because it's \emph{symmetric} monoidal, it
sends \emph{cocommutative} comonoids to \emph{cocommutative} comonoids.  And a
cocommutative comonoid in simplicial vector spaces is the same as a
"simplicial cocommutative coalgebra".  

(I love this kind of stuff, but not everyone does: that's why I
save it for the very end of each Week's Finds.)

So, we've turned our simplicial set S into a simplicial cocommutative
coalgebra F(S).  Now feed this gizmo into the next functor:

N: [simplicial vector spaces] \to  [chain complexes]

By definition, we get the chains on S:

N(F(S)) = C_{<sub>*}</sub>(S)

And thanks to the wonders of functoriality, these chains are blessed
with a comultiplication



% parser failed at source line 887
